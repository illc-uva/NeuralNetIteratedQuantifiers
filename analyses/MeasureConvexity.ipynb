{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement an informational measure of convexity \n",
    "\n",
    "This is based on Shane's implementation of the measure of monotonicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from various_copied import generate_list_models, measure_monotonicity, upward_monotonicity_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_to_int(arr):\n",
    "    \"\"\" Converts a 2-D numpy array of 1s and 0s into integers, assuming each\n",
    "    row is a binary number.  By convention, left-most column is 1, then 2, and\n",
    "    so on, up until 2^(arr.shape[1]).\n",
    "\n",
    "    :param arr: 2D numpy array\n",
    "    :returns: 1D numpy array, length arr.shape[0], containing integers\n",
    "    \"\"\"\n",
    "    return arr.dot(1 << np.arange(arr.shape[-1]))\n",
    "\n",
    "def get_preds(num_arr, num):\n",
    "    \"\"\"\n",
    "    Given an array of ints, and an int, get all predecessors of the\n",
    "    model corresponding to int.\n",
    "    Returns an array of same shape as num_arr, but with bools\n",
    "    \"\"\"\n",
    "    return num_arr & num == num_arr\n",
    "\n",
    "def get_succes(num_arr, num):\n",
    "    return num_arr & num == num\n",
    "\n",
    "def has_true_pred(num_arr, quantifier, num):\n",
    "    \"\"\"\n",
    "    checks if num has at least one true predecessor in num_array for the quantifier\n",
    "    \"\"\"\n",
    "    preds = get_preds(num_arr, num)\n",
    "    return np.any(quantifier * preds)\n",
    "\n",
    "def has_true_contour(num_arr, quantifier, num):\n",
    "    \"\"\"\n",
    "    check if num has at least one true contour (true predecessor and true successor)\n",
    "    \"\"\"\n",
    "    preds = get_preds(num_arr, num)\n",
    "    # find successors\n",
    "    succes = get_succes(num_arr, num)\n",
    "    return np.any(quantifier * preds) and np.any(quantifier * succes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the get_succes function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = generate_list_models(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preds(\n",
    "    binary_to_int(all_models), \n",
    "    binary_to_int(np.array([1, 1, 0]))\n",
    ").reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the only thing I need to modify from Shane's version is that instead of conditionalizing on whether the model has a true submodel, I conditionalize on whether the model has both a true submodel and a true supermodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convexity_entropy(all_models, quantifier):\n",
    "    \"\"\"\n",
    "    Measures degree of convexity of a quantifiers as\n",
    "    1 - H(Q | true_contour) / H(Q) where \n",
    "    (1) H is (conditional) entropy, and \n",
    "    (2) true_contour is whether there's a true submodel and a true supermodel\n",
    "    \"\"\"\n",
    "\n",
    "    quantifier = quantifier.flatten()\n",
    "\n",
    "    if np.all(quantifier) or not np.any(quantifier):\n",
    "        return 1\n",
    "    \n",
    "    # calculate entropy of 1_Q. Average surprisal of quantifier being true.\n",
    "    p_q_true = sum(quantifier) / len(quantifier)\n",
    "    p_q_false = 1 - p_q_true\n",
    "    q_ent = - (p_q_true*np.log2(p_q_true) + p_q_false*np.log2(p_q_false))\n",
    "\n",
    "    # get integers corresponding to each model\n",
    "    model_ints = binary_to_int(all_models)\n",
    "\n",
    "    # vector of length quantifier, \n",
    "    # 1 if that model has a true contour, 0 otherwise\n",
    "    true_contour = np.vectorize(\n",
    "        lambda num: has_true_contour(model_ints, quantifier, num)\n",
    "    )(model_ints).astype(int)\n",
    "\n",
    "    # probability that a random model has a true predecessor (p_pred) and that it doesn't (p_nopred)\n",
    "    p_contour = sum(true_contour) / len(true_contour)\n",
    "    p_nocontour = 1 - p_contour\n",
    "\n",
    "    # prob that quant is true & model has a true pred\n",
    "    q_contour = sum(quantifier * true_contour) / len(quantifier)\n",
    "    # prob that quant is true & model has no true pred\n",
    "    q_nocontour = sum(quantifier * (1 - true_contour)) / len(quantifier)\n",
    "    # prob that quant is false & model has a true pred\n",
    "    noq_contour = sum((1 - quantifier) * true_contour) / len(quantifier)\n",
    "    # prob that quant is false & model has no true pred\n",
    "    noq_nocontour = sum((1 - quantifier) * (1 - true_contour)) / len(quantifier)\n",
    "\n",
    "    # \n",
    "    contour_logs = np.log2([noq_contour, q_contour] / p_contour)\n",
    "    contour_logs[contour_logs == -np.inf] = 0\n",
    "    nocontour_logs = np.log2([noq_nocontour, q_nocontour] / p_nocontour)\n",
    "    nocontour_logs[nocontour_logs == -np.inf] = 0\n",
    "    print(\"noq_nocontour: \", noq_nocontour, \"q_nocontour: \", q_nocontour, \" p_nocontour: \", p_nocontour)\n",
    "    \n",
    "    ent_contour = -np.nansum(np.array([noq_contour, q_contour]) * contour_logs)\n",
    "    ent_nocontour = -np.nansum(np.array([noq_nocontour, q_nocontour]) * nocontour_logs)\n",
    "    print(\"Entropy contour: \", ent_contour, \" Ent no contour: \", ent_nocontour)\n",
    "    cond_ent = ent_contour + ent_nocontour\n",
    "    print(\"True cont: \", true_contour)\n",
    "    # return 0 if q_ent == 0 else 1 - (cond_ent / q_ent)\n",
    "    return 1 - cond_ent / q_ent\n",
    "\n",
    "\n",
    "def measure_convexity(all_models, quantifier, measure=convexity_entropy):\n",
    "    interpretations = [\n",
    "        measure(all_models, quantifier),\n",
    "        measure(all_models, 1 - quantifier),\n",
    "        # downward monotonicity\n",
    "        measure(1 - all_models, 1 - quantifier),\n",
    "        measure(1 - all_models, quantifier)]\n",
    "    return np.max(interpretations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = generate_list_models(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0] Q: [0] cont: 0\n",
      "[1 0 0 0] Q: [1] cont: 1\n",
      "[0 1 0 0] Q: [0] cont: 0\n",
      "[0 0 1 0] Q: [0] cont: 0\n",
      "[0 0 0 1] Q: [0] cont: 0\n",
      "[1 1 0 0] Q: [1] cont: 1\n",
      "[1 0 1 0] Q: [0] cont: 1\n",
      "[1 0 0 1] Q: [0] cont: 1\n",
      "[0 1 1 0] Q: [1] cont: 1\n",
      "[0 1 0 1] Q: [1] cont: 1\n",
      "[0 0 1 1] Q: [1] cont: 1\n",
      "[1 1 1 0] Q: [1] cont: 1\n",
      "[1 1 0 1] Q: [0] cont: 0\n",
      "[1 0 1 1] Q: [1] cont: 1\n",
      "[0 1 1 1] Q: [1] cont: 1\n",
      "[1 1 1 1] Q: [0] cont: 0\n",
      "noq_nocontour:  0.375 q_nocontour:  0.0  p_nocontour:  0.375\n",
      "Entropy contour:  0.4512050593046014  Ent no contour:  -0.0\n",
      "True cont:  [0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0]\n",
      "Monotonicity:  0.5487949406953986\n",
      "True preds:  [0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Convexity:     0.3112781244591327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s1569804\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "random_quant = np.random.randint(0,2,size=(len(all_models),1))\n",
    "# random_quant = ((all_models.sum(axis=1)==1)|(all_models.sum(axis=1)==4)).astype(int)\n",
    "\n",
    "model_ints = binary_to_int(all_models)\n",
    "true_contour = np.vectorize(\n",
    "        lambda num: has_true_contour(model_ints, random_quant.flatten(), num)\n",
    ")(model_ints).astype(int)\n",
    "\n",
    "for idx, model in enumerate(all_models):\n",
    "    print(model, \"Q:\", random_quant[idx], \"cont:\", true_contour[idx])\n",
    "    \n",
    "print(\"Monotonicity: \", convexity_entropy(all_models, random_quant))\n",
    "print(\"Convexity:    \", upward_monotonicity_entropy(all_models, random_quant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the times, for random quantifiers monotonicity and convexity take the same values.\n",
    "- When the smallest and largest models get different values, monotonicity and convexity are identical. \n",
    "    - Monotonicity and convexity are identical when the models that have a true predecessor are the same models that have a true contour, since the only other component (the quantifier's truth itself) is the same.\n",
    "- When they are different, convexity is greater than monotonicity (when considering all possible changes of 1s to 0s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s1569804\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log2\n",
      "C:\\Users\\s1569804\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\s1569804\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in log2\n",
      "C:\\Users\\s1569804\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "for i in range(500):\n",
    "    quant = np.random.randint(0,2,size=(len(all_models),1))\n",
    "    differences.append(measure_convexity(all_models, quant) - measure_monotonicity(all_models, quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = np.array(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00632243, 0.00895973, 0.00603691, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01545698, 0.        ,\n",
       "       0.        , 0.00448651, 0.        , 0.00597827, 0.00866154,\n",
       "       0.        , 0.0083927 , 0.        , 0.        , 0.        ,\n",
       "       0.00588474, 0.00794847, 0.        , 0.        , 0.        ,\n",
       "       0.01164554, 0.        , 0.        , 0.00722388, 0.01814118,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01050007, 0.        , 0.        , 0.        , 0.00653331,\n",
       "       0.        , 0.01809261, 0.        , 0.        , 0.007514  ,\n",
       "       0.        , 0.01036809, 0.0093268 , 0.01257436, 0.01443849,\n",
       "       0.01301536, 0.        , 0.        , 0.        , 0.00314971,\n",
       "       0.0037048 , 0.        , 0.        , 0.00305143, 0.        ,\n",
       "       0.01082841, 0.01350531, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00491597, 0.        ,\n",
       "       0.02131371, 0.        , 0.00518886, 0.        , 0.01203388,\n",
       "       0.00734937, 0.02159569, 0.        , 0.00502713, 0.        ,\n",
       "       0.00840149, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00902422, 0.        , 0.        ,\n",
       "       0.00772774, 0.        , 0.01497968, 0.01862567, 0.01246156,\n",
       "       0.        , 0.        , 0.00874851, 0.        , 0.00516613])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(differences!=0)/len(differences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
